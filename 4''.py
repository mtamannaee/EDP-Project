# READ ME : running LDA of Reviews
import pymysql
from nltk.tokenize import RegexpTokenizer
from stopwords import get_stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim
import community
import networkx as nx
from nltk.tokenize import RegexpTokenizer
from stopwords import get_stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora
import gensim
import pymysql
from gensim.test.utils import datapath

import matplotlib.pyplot as plt

tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = get_stopwords('en')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

# connecting to DBMS
c = pymysql.connect(host="localhost",user='root',password="Alireza123!", db='test')
a= c.cursor()
doc_set=[]

# number of movies in the DBMS
sql='SELECT review from t_rating;'
countrow = a.execute(sql)
print(countrow) # 2678

# creating the document set
for i in range(1,500):
   sql='SELECT review from t_rating where id={};'.format(i)
   countrow = a.execute(sql)
   t_rating = ""+ "".join(a.fetchone())
   doc_set.append(t_rating)

# printing document set
for i in range(doc_set.__len__()):
    print(" Review {}:{} \n".format(i,doc_set[i]))




# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:
    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]

    # add tokens to list
    texts.append(stemmed_tokens)

# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]
print(corpus)

# generate LDA model
model = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary)

# show Topics generated by model
for idx, topic in model.print_topics(-1):
    print('Topic {} : \n Words: {} \n'.format(idx, topic))
# OR
topics = model.show_topics()
for topic in topics:
    print (topic)

# Get the topic distribution for the given document.
print ("get_document_topics : ")
for d in corpus:
    # get_document_topics for a document
    print (" document {} [Topic#, probability>0.01]: ".format(corpus.index(d)+1), model.get_document_topics(d,minimum_probability=0.01))

# Graph Representation and Calculations

G = nx.Graph() #networkx graph
Gc = nx.erdos_renyi_graph(30,0.05)#networkx graph
partition = community.best_partition(Gc)

topics_words = model.print_topics(-1) # tuples(topicid,'sum(probabilities*"words")')   #print(topics_words)

# 3 Top TOPICS
doc_3top_topic=[] #list of lists, each inner list relates the the document number being equal to the list index
for text in texts:
    doc_id = texts.index(text)
    G.add_node(doc_id)
    topicList = model.get_document_topics(dictionary.doc2bow(text))
    sorted_topicList = sorted(topicList,key=lambda l:l[1], reverse=True)
    only_topic_num= [i[0] for i in sorted_topicList[:3]]
    doc_3top_topic.append(only_topic_num)
#print(doc_3top_topic)

# 2 Top TOPICS
doc_2top_topic=[] #list of lists, each inner list relates the the document number being equal to the list index
for text in texts:
    doc_id = texts.index(text)
    G.add_node(doc_id)
    topicList = model.get_document_topics(dictionary.doc2bow(text))
    sorted_topicList = sorted(topicList,key=lambda l:l[1], reverse=True)
    only_topic_num= [i[0] for i in sorted_topicList[:2]]
    doc_3top_topic.append(only_topic_num)
#print(doc_2top_topic)

#for i in range (texts.__len__()):
#    G.add_node(i)

# # Normal Homo Graph :
# for a in range (doc_3top_topic.__len__()):
#     for b in range(a):
#         if ( bool(set(doc_3top_topic[a]) & set(doc_3top_topic[b])) and a!=b):
#             G.add_edge(a, b)
#
# nx.draw(G, style='dashed',with_labels=True, font_size=9)
# plt.savefig("LDA-3top-Reviews.png")
# plt.show()

# Graph clusters :
size = float(len(partition.values()))
pos = nx.spring_layout(Gc)
count=0
for com in set(partition.values()):
    count=count+1
    list_nodes = [nodes for nodes in partition.keys() if partition[nodes]==com]
    nx.draw_networkx_nodes(Gc,pos,list_nodes,node_size=20,node_color=str(count/size))

nx.draw_networkx_edges(Gc,pos,alpha=0.5)
plt.savefig("LDA_clus_Reviews.png")
plt.show()