# READ ME : running LDA of movie description for classifying the movies based on description
import pymysql
from nltk.tokenize import RegexpTokenizer
from stopwords import get_stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = get_stopwords('en')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

# connecting to DBMS
c = pymysql.connect(host="localhost",user='root',password="Alireza123!", db='test')
a= c.cursor()
doc_set=[]

# number of movies in the DBMS
sql='SELECT desc_ from t_movie;'
countrow = a.execute(sql)
print(countrow) # 2678

# creating the document set
for i in range(1,51):
   sql='SELECT desc_ from t_movie where M_id={};'.format(i)
   countrow = a.execute(sql)
   t_movie = ""+ "".join(a.fetchone())
   doc_set.append(t_movie)

# printing document set
for i in range(doc_set.__len__()):
    print(" Movie {} Description :{} \n".format(i,doc_set[i]))




# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:
    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]

    # add tokens to list
    texts.append(stemmed_tokens)

# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]
print(corpus)

# generate LDA model
model = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary)

# show Topics generated by model
for idx, topic in model.print_topics(-1):
    print('Topic {} : \n Words: {} \n'.format(idx, topic))
# OR
topics = model.show_topics()
for topic in topics:
    print (topic)

# Get the topic distribution for the given document.
print ("get_document_topics : ")
for d in corpus:
    # get_document_topics for a document
    print (" document {} [Topic#, probability>0.01]: ".format(corpus.index(d)+1), model.get_document_topics(d,minimum_probability=0.01))
